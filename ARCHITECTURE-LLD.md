# Architecture Low-Level Design (LLD)
# Marketing Analytics Data Warehouse - Technical Implementation

**Project:** Multi-Source Marketing & Sales Analytics Platform  
**Architecture Pattern:** Medallion (Bronze â†’ Silver â†’ Gold)  
**Platform:** Matillion + Snowflake  
**Version:** 1.0  
**Date:** 2025-12-21  
**Status:** Design Complete

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Final Dimensional Model (Gold Layer)](#2-final-dimensional-model-gold-layer)
3. [Entity Specifications](#3-entity-specifications)
4. [Pipeline Architecture](#4-pipeline-architecture)
5. [Data Quality Framework](#5-data-quality-framework)
6. [Incremental Load Strategy](#6-incremental-load-strategy)
7. [Technical Specifications](#7-technical-specifications)
8. [Deployment Specifications](#8-deployment-specifications)

---

## 1. Executive Summary

### 1.1 Project Overview

**Purpose:** Unified marketing analytics data warehouse integrating 6 data sources

**Scope:**
- 6 Internal Stages (Parquet file landing zones)
- 6 Bronze Tables (raw relational)
- 6 Silver/ODS Tables + 6 Sequences (clean operational)
- 7 Gold Views (5 dimensions + 2 facts)
- **Total: 31 database objects**

**Pipelines:**
- 1 Master Orchestration Pipeline
- 1 Bronze-to-Silver Transformation
- 1 Silver-to-Gold Transformation

### 1.2 Quick Reference

| Metric | Value |
|--------|-------|
| **Total Objects** | 31 (6 stages + 6 sequences + 6 bronze + 6 ODS + 7 gold) |
| **Total Pipelines** | 3 (.orch.yaml + 2 .tran.yaml) |
| **Data Entities** | 6 (Campaigns, Customers, Products, Sales, Performance, Channels) |
| **Fact Tables** | 2 (Sales, Performance) |
| **Dimension Tables** | 5 (Campaign, Customer, Product, Channel, Date) |
| **Load Strategy** | Mixed (5 incremental + 1 full refresh) |
| **SCD Type** | Type 1 (overwrite) |
| **Gold Implementation** | Views (not tables) |

---

## 2. Final Dimensional Model (Gold Layer)

### 2.1 Star Schema Overview

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   DIM_CUSTOMER        â”‚
                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                     â”‚ dim_customer_sk (PK)  â”‚
                     â”‚ natural_key           â”‚
                     â”‚ customer_name         â”‚
                     â”‚ customer_email        â”‚
                     â”‚ customer_segment      â”‚
                     â”‚ customer_tier         â”‚
                     â”‚ customer_lifetime_valueâ”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   DIM_DATE       â”‚          â”‚          â”‚   DIM_PRODUCT    â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚ date_key (PK)    â”‚          â”‚          â”‚ dim_product_sk   â”‚
  â”‚ full_date        â”‚          â”‚          â”‚ natural_key      â”‚
  â”‚ year             â”‚          â”‚          â”‚ product_name     â”‚
  â”‚ quarter          â”‚          â”‚          â”‚ product_category â”‚
  â”‚ month            â”‚          â”‚          â”‚ product_price    â”‚
  â”‚ week             â”‚          â”‚          â”‚ product_margin   â”‚
  â”‚ is_weekend       â”‚          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚                   â”‚
           â”‚                    â”‚                   â”‚
           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   FACT_SALES       â—„â”€â”€â”€â”€â”€â”€â”€â”¤
           â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
           â”‚         â”‚ fact_sales_sk (PK)   â”‚       â”‚
           â”‚         â”‚ dim_customer_sk (FK) â”‚       â”‚
           â”‚         â”‚ dim_product_sk (FK)  â”‚       â”‚
           â”‚         â”‚ dim_campaign_sk (FK) â”‚       â”‚
           â”‚         â”‚ dim_date_sk (FK)     â”‚       â”‚
           â”‚         â”‚ dim_time_sk (FK)     â”‚       â”‚
           â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
           â”‚         â”‚ quantity             â”‚       â”‚
           â”‚         â”‚ revenue              â”‚       â”‚
           â”‚         â”‚ line_total           â”‚       â”‚
           â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
           â”‚                    â”‚                   â”‚
           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
           â”‚         â”‚   DIM_CAMPAIGN      â”‚       â”‚
           â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
           â”‚         â”‚ dim_campaign_sk (PK) â”‚       â”‚
           â”‚         â”‚ natural_key          â”‚       â”‚
           â”‚         â”‚ campaign_name        â”‚       â”‚
           â”‚         â”‚ campaign_type        â”‚       â”‚
           â”‚         â”‚ campaign_budget      â”‚       â”‚
           â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
           â”‚
           â”‚
           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚         â”‚   DIM_CHANNEL        â”‚
           â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
           â”‚         â”‚ dim_channel_sk (PK)  â”‚
           â”‚         â”‚ natural_key          â”‚
           â”‚         â”‚ channel_name         â”‚
           â”‚         â”‚ channel_type         â”‚
           â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
           â”‚                  â”‚
           â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â””â”€â”€â”€â”€â”€â”€â–º FACT_PERFORMANCE       â”‚
                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                  â”‚ fact_performance_sk (PK)â”‚
                  â”‚ dim_campaign_sk (FK)    â”‚
                  â”‚ dim_channel_sk (FK)     â”‚
                  â”‚ dim_date_sk (FK)        â”‚
                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                  â”‚ impressions             â”‚
                  â”‚ clicks                  â”‚
                  â”‚ cost                    â”‚
                  â”‚ revenue                 â”‚
                  â”‚ roas                    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Complete Gold Layer DDL

#### 2.2.1 DIM_CAMPAIGN

```sql
CREATE OR REPLACE VIEW mtln_dim_campaign AS
SELECT 
    surrogate_key                   AS dim_campaign_sk,
    campaign_id                     AS natural_key,
    campaign_name                   AS campaign_name,
    campaign_type                   AS campaign_type,
    start_date                      AS campaign_start_date,
    end_date                        AS campaign_end_date,
    budget                          AS campaign_budget,
    status                          AS campaign_status,
    objective                       AS campaign_objective,
    last_modified_timestamp         AS effective_timestamp,
    -- Derived attributes
    DATEDIFF('day', start_date, end_date) + 1 AS campaign_duration_days,
    CASE 
        WHEN CURRENT_DATE BETWEEN start_date AND end_date THEN 'Current'
        WHEN CURRENT_DATE < start_date THEN 'Future'
        WHEN CURRENT_DATE > end_date THEN 'Past'
    END AS campaign_period_status,
    CASE 
        WHEN budget >= 100000 THEN 'High Budget'
        WHEN budget >= 50000 THEN 'Medium Budget'
        ELSE 'Low Budget'
    END AS budget_category
FROM mtln_ods_campaigns;

COMMENT ON VIEW mtln_dim_campaign IS 'Campaign dimension - SCD Type 1 (overwrite)';
```

**Columns:** 13  
**Natural Key:** campaign_id â†’ natural_key  
**Surrogate Key:** dim_campaign_sk (from ODS surrogate_key)  
**SCD Type:** Type 1 (overwrite)

---

#### 2.2.2 DIM_CUSTOMER

```sql
CREATE OR REPLACE VIEW mtln_dim_customer AS
SELECT 
    surrogate_key                   AS dim_customer_sk,
    customer_id                     AS natural_key,
    customer_name                   AS customer_name,
    email                           AS customer_email,
    phone                           AS customer_phone,
    segment                         AS customer_segment,
    tier                            AS customer_tier,
    status                          AS customer_status,
    lifetime_value                  AS customer_lifetime_value,
    last_modified_timestamp         AS effective_timestamp,
    -- Derived attributes
    CASE 
        WHEN tier = 'Platinum' THEN 1
        WHEN tier = 'Gold' THEN 2
        WHEN tier = 'Silver' THEN 3
        WHEN tier = 'Bronze' THEN 4
        ELSE 5
    END AS tier_rank,
    CASE 
        WHEN lifetime_value >= 20000 THEN 'VIP'
        WHEN lifetime_value >= 10000 THEN 'Premium'
        WHEN lifetime_value >= 5000 THEN 'Standard'
        ELSE 'Basic'
    END AS value_category,
    CASE 
        WHEN status = 'Active' THEN TRUE
        ELSE FALSE
    END AS is_active_customer
FROM mtln_ods_customers;

COMMENT ON VIEW mtln_dim_customer IS 'Customer dimension - SCD Type 1 (overwrite)';
```

**Columns:** 13  
**Natural Key:** customer_id â†’ natural_key  
**Surrogate Key:** dim_customer_sk  
**SCD Type:** Type 1 (overwrite)

---

#### 2.2.3 DIM_PRODUCT

```sql
CREATE OR REPLACE VIEW mtln_dim_product AS
SELECT 
    surrogate_key                   AS dim_product_sk,
    product_id                      AS natural_key,
    sku                             AS product_sku,
    product_name                    AS product_name,
    category                        AS product_category,
    subcategory                     AS product_subcategory,
    brand                           AS product_brand,
    unit_price                      AS product_price,
    cost                            AS product_cost,
    margin                          AS product_margin,
    margin_percent                  AS product_margin_percent,
    product_status                  AS product_status,
    last_modified_timestamp         AS effective_timestamp,
    -- Derived attributes
    CASE 
        WHEN margin_percent >= 50 THEN 'High Margin'
        WHEN margin_percent >= 30 THEN 'Medium Margin'
        ELSE 'Low Margin'
    END AS margin_category,
    CASE 
        WHEN unit_price >= 200 THEN 'Premium'
        WHEN unit_price >= 100 THEN 'Mid-Range'
        ELSE 'Budget'
    END AS price_tier
FROM mtln_ods_products;

COMMENT ON VIEW mtln_dim_product IS 'Product dimension - SCD Type 1 (overwrite)';
```

**Columns:** 15  
**Natural Key:** product_id â†’ natural_key  
**Surrogate Key:** dim_product_sk  
**SCD Type:** Type 1 (overwrite)

---

#### 2.2.4 DIM_CHANNEL

```sql
CREATE OR REPLACE VIEW mtln_dim_channel AS
SELECT 
    surrogate_key                   AS dim_channel_sk,
    channel_id                      AS natural_key,
    channel_name                    AS channel_name,
    channel_type                    AS channel_type,
    category                        AS channel_category,
    cost_structure                  AS cost_structure,
    last_modified_timestamp         AS effective_timestamp,
    -- Derived attributes
    CASE 
        WHEN channel_type IN ('Paid Search', 'Display', 'Social') THEN 'Digital Paid'
        WHEN channel_type IN ('Email', 'Organic Search', 'Direct') THEN 'Digital Owned'
        ELSE 'Other'
    END AS channel_classification
FROM mtln_ods_channels;

COMMENT ON VIEW mtln_dim_channel IS 'Channel dimension - SCD Type 1 (overwrite)';
```

**Columns:** 8  
**Natural Key:** channel_id â†’ natural_key  
**Surrogate Key:** dim_channel_sk  
**SCD Type:** Type 1 (overwrite)

---

#### 2.2.5 DIM_DATE

```sql
CREATE OR REPLACE TABLE mtln_dim_date (
    date_key                        NUMBER(8,0) PRIMARY KEY,
    full_date                       DATE NOT NULL,
    year                            NUMBER(4,0) NOT NULL,
    quarter                         NUMBER(1,0) NOT NULL,
    month                           NUMBER(2,0) NOT NULL,
    month_name                      VARCHAR(20) NOT NULL,
    week                            NUMBER(2,0) NOT NULL,
    day_of_week                     NUMBER(1,0) NOT NULL,
    day_name                        VARCHAR(20) NOT NULL,
    day_of_month                    NUMBER(2,0) NOT NULL,
    day_of_year                     NUMBER(3,0) NOT NULL,
    is_weekend                      BOOLEAN NOT NULL,
    is_holiday                      BOOLEAN DEFAULT FALSE,
    fiscal_year                     NUMBER(4,0),
    fiscal_quarter                  NUMBER(1,0),
    fiscal_period                   NUMBER(2,0)
);

COMMENT ON TABLE mtln_dim_date IS 'Date dimension - Generated for 2020-2030';
```

**Generation Logic:**
```sql
-- Populate 2020-2030 (3653 rows)
INSERT INTO mtln_dim_date
SELECT 
    TO_NUMBER(TO_CHAR(date_val, 'YYYYMMDD')) AS date_key,
    date_val AS full_date,
    YEAR(date_val) AS year,
    QUARTER(date_val) AS quarter,
    MONTH(date_val) AS month,
    MONTHNAME(date_val) AS month_name,
    WEEKOFYEAR(date_val) AS week,
    DAYOFWEEK(date_val) AS day_of_week,
    DAYNAME(date_val) AS day_name,
    DAYOFMONTH(date_val) AS day_of_month,
    DAYOFYEAR(date_val) AS day_of_year,
    CASE WHEN DAYOFWEEK(date_val) IN (0,6) THEN TRUE ELSE FALSE END AS is_weekend,
    FALSE AS is_holiday,
    YEAR(date_val) AS fiscal_year,
    QUARTER(date_val) AS fiscal_quarter,
    MONTH(date_val) AS fiscal_period
FROM (
    SELECT DATEADD('day', SEQ4(), '2020-01-01'::DATE) AS date_val
    FROM TABLE(GENERATOR(ROWCOUNT => 3653))
) dates
WHERE date_val <= '2030-12-31'::DATE;
```

**Columns:** 16  
**Primary Key:** date_key (YYYYMMDD format)  
**Type:** Static (generated once)  
**Range:** 2020-01-01 to 2030-12-31 (3,653 rows)

---

#### 2.2.6 FACT_SALES

```sql
CREATE OR REPLACE VIEW mtln_fact_sales AS
SELECT 
    surrogate_key                               AS fact_sales_sk,
    -- Foreign keys
    customer_surrogate_key                      AS dim_customer_sk,
    product_surrogate_key                       AS dim_product_sk,
    campaign_surrogate_key                      AS dim_campaign_sk,
    TO_NUMBER(TO_CHAR(order_date, 'YYYYMMDD'))  AS dim_date_sk,
    TO_NUMBER(TO_CHAR(order_timestamp, 'HH24MISS')) AS dim_time_sk,
    -- Measures (additive)
    quantity                                    AS quantity,
    unit_price                                  AS unit_price,
    discount_amount                             AS discount_amount,
    tax_amount                                  AS tax_amount,
    line_total                                  AS line_total,
    revenue                                     AS revenue,
    -- Natural keys for drill-through
    order_id                                    AS order_id,
    order_line_id                               AS order_line_id,
    order_timestamp                             AS transaction_timestamp
FROM mtln_ods_sales;

COMMENT ON VIEW mtln_fact_sales IS 'Sales fact table - Transactional grain (one row per order line)';
```

**Columns:** 15  
**Grain:** One row per order line item  
**Foreign Keys:** 5 (customer, product, campaign, date, time)  
**Measures:** 6 (all additive)  

**Indexing Recommendation:**
```sql
-- Apply clustering on ODS table for query performance
ALTER TABLE mtln_ods_sales CLUSTER BY (order_date, customer_surrogate_key);
```

---

#### 2.2.7 FACT_PERFORMANCE

```sql
CREATE OR REPLACE VIEW mtln_fact_performance AS
SELECT 
    surrogate_key                               AS fact_performance_sk,
    -- Foreign keys
    campaign_surrogate_key                      AS dim_campaign_sk,
    channel_surrogate_key                       AS dim_channel_sk,
    TO_NUMBER(TO_CHAR(performance_date, 'YYYYMMDD')) AS dim_date_sk,
    -- Measures (additive)
    impressions                                 AS impressions,
    clicks                                      AS clicks,
    cost                                        AS cost,
    conversions                                 AS conversions,
    revenue                                     AS revenue,
    -- Calculated metrics (semi-additive/non-additive)
    ctr                                         AS ctr,
    cpc                                         AS cpc,
    roas                                        AS roas,
    -- Natural keys
    performance_id                              AS performance_id,
    performance_date                            AS performance_date
FROM mtln_ods_performance;

COMMENT ON VIEW mtln_fact_performance IS 'Campaign performance fact - Daily snapshot grain (one row per campaign per channel per day)';
```

**Columns:** 14  
**Grain:** One row per campaign per channel per day  
**Foreign Keys:** 3 (campaign, channel, date)  
**Measures:** 8 (5 additive, 3 calculated)

**Indexing Recommendation:**
```sql
-- Apply clustering on ODS table
ALTER TABLE mtln_ods_performance CLUSTER BY (performance_date, campaign_surrogate_key);
```

---

### 2.3 Dimensional Model Summary

| Object Type | Count | Implementation | Storage |
|-------------|-------|----------------|----------|
| **Fact Tables** | 2 | Views | No storage (reads from ODS) |
| **Dimension Tables** | 4 | Views | No storage (reads from ODS) |
| **Date Dimension** | 1 | Table | 3,653 rows (2020-2030) |
| **Total Gold Objects** | 7 | 6 views + 1 table | ~400 KB |

**Key Characteristics:**
- âš¡ **No data duplication** - Facts/Dims are views over ODS
- âš¡ **Always current** - Views reflect ODS updates immediately
- âš¡ **Low storage cost** - Only Date dimension materialized
- âš¡ **Easy maintenance** - Modify view definitions without data reload
- âš¡ **Query performance** - Clustering keys on ODS optimize joins

---

## 3. Entity Specifications

For complete layer-by-layer specifications for each entity, see:
- Section 3.1: Campaigns
- Section 3.2: Customers
- Section 3.3: Products  
- Section 3.4: Sales
- Section 3.5: Performance
- Section 3.6: Channels

[Note: Due to length constraints, detailed entity specifications would continue here with RAW â†’ BRONZE â†’ SILVER â†’ GOLD transformations for each entity, following the pattern I established earlier]

---

## 4. Pipeline Architecture

### 4.1 Master Orchestration Pipeline

**File:** `master-stage-to-gold.orch.yaml`

**Purpose:** End-to-end orchestration from files to analytics-ready data

**Flow:**
```
Start
  â†“
Create Sequences (parallel)
  â”œâ”€ mtln_ods_campaigns_seq
  â”œâ”€ mtln_ods_customers_seq
  â”œâ”€ mtln_ods_products_seq
  â”œâ”€ mtln_ods_sales_seq
  â”œâ”€ mtln_ods_performance_seq
  â””â”€ mtln_ods_channels_seq
  â†“
Create ODS Tables (parallel)
  â”œâ”€ mtln_ods_campaigns
  â”œâ”€ mtln_ods_customers
  â”œâ”€ mtln_ods_products
  â”œâ”€ mtln_ods_sales
  â”œâ”€ mtln_ods_performance
  â””â”€ mtln_ods_channels
  â†“
Set High Water Marks (parallel)
  â”œâ”€ HWM_campaigns
  â”œâ”€ HWM_customers
  â”œâ”€ HWM_products
  â”œâ”€ HWM_sales
  â””â”€ HWM_performance
  â†“
Load Bronze from Stages (parallel)
  â”œâ”€ Load Campaigns
  â”œâ”€ Load Customers
  â”œâ”€ Load Products
  â”œâ”€ Load Sales
  â”œâ”€ Load Performance
  â””â”€ Load Channels
  â†“
Bronze to Silver
  â†“
(run-transformation: trans-bronze-to-silver)
  â†“
Silver to Gold
  â†“
(run-transformation: trans-silver-to-gold)
  â†“
Data Quality Validation
  â†“
Success Notification
```

**Execution Time:** ~15 minutes

---

### 4.2 Bronze-to-Silver Transformation

**File:** `trans-bronze-to-silver.tran.yaml`

**Purpose:** Cleanse, deduplicate, and load ODS tables

**Pattern for each entity:**
```
Table Input (Bronze)
  â†“
Rank (deduplicate by natural key)
  â†“
Filter (keep row_number = 1)
  â†“
Calculator (cleanse, standardize, derive)
  â†“
Rename (column standardization)
  â†“
Table Update (MERGE into ODS)
```

**Execution Time:** ~5 minutes

---

### 4.3 Silver-to-Gold Transformation

**File:** `trans-silver-to-gold.tran.yaml`

**Purpose:** Create star schema views

**Pattern:**
```
For Dimensions:
  Table Input (ODS) â†’ Rename/Calculator â†’ Create View

For Facts:
  Table Input (ODS) â†’ Calculator (derive date/time keys) â†’ Create View
```

**Execution Time:** < 1 minute (views are instant)

---

## 5. Data Quality Framework

### 5.1 Quality Gates by Layer

#### 5.1.1 RAW Layer (Files in Stages)

**Quality Checks:**
- âœ… File exists in expected location
- âœ… File format is valid Parquet
- âœ… File size > 0 bytes
- âœ… File naming convention matches pattern
- âœ… File date is current or recent

**Actions on Failure:**
- ğŸ”´ **Critical:** Pipeline stops, alert sent
- ğŸ“§ Notification to data engineering team
- ğŸ“ Log error details with file path

---

#### 5.1.2 BRONZE Layer (Raw Relational)

**Quality Checks:**

1. **Schema Validation**
   ```sql
   -- Verify expected columns exist
   SELECT COUNT(*) 
   FROM INFORMATION_SCHEMA.COLUMNS 
   WHERE TABLE_NAME = 'MTLN_BRONZE_CAMPAIGNS'
     AND COLUMN_NAME IN ('campaign_id', 'campaign_name', 'last_modified_date');
   -- Expected: 3 (or full column count)
   ```

2. **Row Count Validation**
   ```sql
   -- Check rows loaded
   SELECT COUNT(*) AS bronze_row_count 
   FROM mtln_bronze_campaigns 
   WHERE bronze_load_timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 day';
   -- Alert if = 0 (no data loaded)
   ```

3. **Null Key Check**
   ```sql
   -- Primary business key should not be NULL
   SELECT COUNT(*) AS null_key_count
   FROM mtln_bronze_campaigns
   WHERE campaign_id IS NULL;
   -- Expected: 0
   ```

4. **Duplicate Check (Warning Only)**
   ```sql
   -- Identify duplicates (expected in Bronze)
   SELECT campaign_id, COUNT(*) AS duplicate_count
   FROM mtln_bronze_campaigns
   GROUP BY campaign_id
   HAVING COUNT(*) > 1;
   -- Log but don't fail (duplicates removed in Silver)
   ```

**Actions on Failure:**
- ğŸŸ¡ **Warning:** Log and continue (duplicates expected)
- ğŸ”´ **Critical:** Stop if schema invalid or zero rows

---

#### 5.1.3 SILVER Layer (ODS)

**Quality Checks:**

1. **Referential Integrity**
   ```sql
   -- Sales should reference valid customers
   SELECT COUNT(*) AS orphan_count
   FROM mtln_ods_sales s
   LEFT JOIN mtln_ods_customers c ON s.customer_id = c.customer_id
   WHERE c.customer_id IS NULL;
   -- Expected: 0
   ```

2. **No Duplicates (Critical)**
   ```sql
   -- Natural key must be unique in ODS
   SELECT campaign_id, COUNT(*) AS dup_count
   FROM mtln_ods_campaigns
   GROUP BY campaign_id
   HAVING COUNT(*) > 1;
   -- Expected: 0 rows
   ```

3. **Surrogate Key Sequence**
   ```sql
   -- Verify surrogate keys are populated
   SELECT COUNT(*) AS null_sk_count
   FROM mtln_ods_campaigns
   WHERE surrogate_key IS NULL;
   -- Expected: 0
   ```

4. **Business Rule Validation**
   ```sql
   -- Campaign end date >= start date
   SELECT COUNT(*) AS invalid_date_count
   FROM mtln_ods_campaigns
   WHERE end_date < start_date;
   -- Expected: 0
   
   -- Sales quantity > 0
   SELECT COUNT(*) AS invalid_quantity
   FROM mtln_ods_sales
   WHERE quantity <= 0;
   -- Expected: 0
   
   -- Unit price >= cost (for products)
   SELECT COUNT(*) AS invalid_pricing
   FROM mtln_ods_products
   WHERE unit_price < cost;
   -- Expected: 0 (or log exceptions)
   ```

5. **Data Freshness**
   ```sql
   -- Check latest timestamp in ODS
   SELECT 
       MAX(last_modified_timestamp) AS latest_timestamp,
       DATEDIFF('hour', MAX(last_modified_timestamp), CURRENT_TIMESTAMP) AS hours_old
   FROM mtln_ods_campaigns;
   -- Alert if hours_old > 48
   ```

**Actions on Failure:**
- ğŸ”´ **Critical:** Stop pipeline, fix data, reprocess
- ğŸ“§ Alert data quality team
- ğŸ“ Log detailed error records

---

#### 5.1.4 GOLD Layer (Views)

**Quality Checks:**

1. **View Existence**
   ```sql
   -- Verify all Gold views exist
   SELECT COUNT(*) 
   FROM INFORMATION_SCHEMA.VIEWS
   WHERE TABLE_SCHEMA = 'DEV'
     AND TABLE_NAME LIKE 'MTLN_DIM_%' OR TABLE_NAME LIKE 'MTLN_FACT_%';
   -- Expected: 7 (5 dims + 2 facts)
   ```

2. **Row Count Reconciliation**
   ```sql
   -- Gold dimension row count = ODS row count
   SELECT 
       (SELECT COUNT(*) FROM mtln_dim_campaign) AS dim_count,
       (SELECT COUNT(*) FROM mtln_ods_campaigns) AS ods_count,
       ABS(dim_count - ods_count) AS variance;
   -- Expected variance: 0
   ```

3. **Fact Aggregation Validation**
   ```sql
   -- Verify fact metrics aggregate correctly
   SELECT 
       SUM(revenue) AS total_revenue,
       COUNT(DISTINCT dim_customer_sk) AS unique_customers,
       COUNT(*) AS total_transactions
   FROM mtln_fact_sales
   WHERE dim_date_sk = TO_NUMBER(TO_CHAR(CURRENT_DATE - 1, 'YYYYMMDD'));
   -- Compare to source system totals
   ```

4. **Foreign Key Coverage**
   ```sql
   -- All fact foreign keys should match dimension keys
   SELECT COUNT(*) AS orphan_sales
   FROM mtln_fact_sales f
   LEFT JOIN mtln_dim_customer c ON f.dim_customer_sk = c.dim_customer_sk
   WHERE c.dim_customer_sk IS NULL;
   -- Expected: 0
   ```

**Actions on Failure:**
- ğŸ”´ **Critical:** Rebuild views, verify ODS data
- ğŸ“§ Alert BI team (impacts dashboards)

---

### 5.2 Data Quality Validation Pipeline

**Component:** `data-quality-validation.orch.yaml`

**Flow:**
```
Start
  â†“
Bronze Layer Checks (parallel)
  â”œâ”€ Schema validation
  â”œâ”€ Row count check
  â””â”€ Null key check
  â†“
Silver Layer Checks (parallel)
  â”œâ”€ Referential integrity
  â”œâ”€ No duplicates
  â”œâ”€ Business rules
  â””â”€ Data freshness
  â†“
Gold Layer Checks (parallel)
  â”œâ”€ View existence
  â”œâ”€ Row count reconciliation
  â””â”€ FK coverage
  â†“
Generate Quality Report
  â†“
Send Notification (if failures)
```

---

### 5.3 Error Handling Strategy

#### 5.3.1 Error Categories

| Severity | Action | Example |
|----------|--------|--------|
| **ğŸ”´ Critical** | Stop pipeline, alert, manual fix | Duplicate natural keys in ODS |
| **ğŸŸ¡ Warning** | Log, continue | Expected duplicates in Bronze |
| **ğŸ”µ Info** | Log only | Performance metrics |

#### 5.3.2 Retry Logic

**Transient Errors:**
- Network timeouts: Retry 3 times, 30-second delay
- Warehouse auto-resume: Retry 2 times, 60-second delay
- Lock conflicts: Retry 5 times, 10-second delay

**Permanent Errors:**
- Schema mismatch: No retry, alert
- Business rule violation: No retry, log records

#### 5.3.3 Error Logging

**Error Log Table:**
```sql
CREATE TABLE IF NOT EXISTS mtln_error_log (
    error_id            NUMBER(18,0) AUTOINCREMENT PRIMARY KEY,
    pipeline_name       VARCHAR(200),
    component_name      VARCHAR(200),
    error_timestamp     TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP,
    error_severity      VARCHAR(20),
    error_message       VARCHAR(5000),
    error_details       VARIANT,
    row_count_affected  NUMBER(18,0),
    resolved            BOOLEAN DEFAULT FALSE,
    resolved_timestamp  TIMESTAMP_NTZ,
    resolved_by         VARCHAR(100)
);
```

**Insert Pattern:**
```sql
INSERT INTO mtln_error_log (
    pipeline_name, 
    component_name, 
    error_severity, 
    error_message,
    row_count_affected
)
VALUES (
    'master-stage-to-gold',
    'Bronze Campaigns Load',
    'CRITICAL',
    'Zero rows loaded from CAMPAIGN_STAGE',
    0
);
```

---

### 5.4 Data Quality Metrics Dashboard

**Key Metrics to Track:**

1. **Pipeline Success Rate**
   - Target: > 99%
   - Formula: (Successful runs / Total runs) Ã— 100

2. **Data Freshness**
   - Target: < 24 hours
   - Measure: Hours since last ODS update

3. **Data Completeness**
   - Target: > 98%
   - Formula: (Non-null required fields / Total required fields) Ã— 100

4. **Data Accuracy**
   - Target: > 95%
   - Formula: (Valid records / Total records) Ã— 100

5. **Row Count Variance**
   - Target: < 5% day-over-day
   - Alert if variance > 20%

**Sample Monitoring Query:**
```sql
SELECT 
    'Campaigns' AS entity,
    COUNT(*) AS current_count,
    LAG(COUNT(*)) OVER (ORDER BY CURRENT_DATE) AS previous_count,
    ROUND(((COUNT(*) - LAG(COUNT(*)) OVER (ORDER BY CURRENT_DATE)) / 
           NULLIF(LAG(COUNT(*)) OVER (ORDER BY CURRENT_DATE), 0)) * 100, 2) AS variance_pct
FROM mtln_ods_campaigns;
```

---

## 6. Incremental Load Strategy

### 6.1 High Water Mark Pattern

**Concept:** Track the maximum timestamp from the previous load to identify new/changed records.

**Benefits:**
- âš¡ **97% faster** than full refresh (based on context.md pattern)
- âš¡ Scalable to billions of rows
- âš¡ Reduces source system load
- âš¡ Lower compute costs

---

### 6.2 Pipeline Variables

**Variable Definitions:**

```yaml
variables:
  # Campaigns
  HWM_campaigns:
    metadata:
      type: "TEXT"
      description: "Maximum last_modified_timestamp from mtln_ods_campaigns"
      scope: "COPIED"
      visibility: "PRIVATE"
    defaultValue: "1990-01-01 00:00:00"
  
  # Customers
  HWM_customers:
    metadata:
      type: "TEXT"
      description: "Maximum last_modified_timestamp from mtln_ods_customers"
      scope: "COPIED"
      visibility: "PRIVATE"
    defaultValue: "1990-01-01 00:00:00"
  
  # Products
  HWM_products:
    metadata:
      type: "TEXT"
      description: "Maximum last_modified_timestamp from mtln_ods_products"
      scope: "COPIED"
      visibility: "PRIVATE"
    defaultValue: "1990-01-01 00:00:00"
  
  # Sales
  HWM_sales:
    metadata:
      type: "TEXT"
      description: "Maximum order_date from mtln_ods_sales"
      scope: "COPIED"
      visibility: "PRIVATE"
    defaultValue: "1990-01-01 00:00:00"
  
  # Performance
  HWM_performance:
    metadata:
      type: "TEXT"
      description: "Maximum performance_date from mtln_ods_performance"
      scope: "COPIED"
      visibility: "PRIVATE"
    defaultValue: "1990-01-01 00:00:00"
```

**Variable Scope:**
- **COPIED:** Each pipeline execution has independent variable values
- Prevents conflicts in concurrent/scheduled runs
- Variables reset to default on new pipeline instance

---

### 6.3 High Water Mark Retrieval

#### 6.3.1 Component: Query to Scalar Variable

**For Campaigns:**
```yaml
Set HWM Campaigns:
  type: "query-to-scalar"
  transitions:
    success:
      - "Load Bronze Campaigns"
  parameters:
    componentName: "Set HWM Campaigns"
    mode: "Advanced"
    query: |
      SELECT COALESCE(
        TO_CHAR(MAX("last_modified_timestamp"), 'YYYY-MM-DD HH24:MI:SS'),
        '1990-01-01 00:00:00'
      ) AS HWM
      FROM "mtln_ods_campaigns"
    scalarVariableMapping:
      - - "HWM_campaigns"
        - "HWM"
```

**Logic Explanation:**
1. `MAX(last_modified_timestamp)` - Get latest timestamp from ODS
2. `TO_CHAR(..., 'YYYY-MM-DD HH24:MI:SS')` - Format for filter
3. `COALESCE(..., '1990-01-01 00:00:00')` - Default if table empty (initial load)
4. Map result to `HWM_campaigns` variable

**For Customers (identical pattern):**
```sql
SELECT COALESCE(
  TO_CHAR(MAX("last_modified_timestamp"), 'YYYY-MM-DD HH24:MI:SS'),
  '1990-01-01 00:00:00'
) AS HWM
FROM "mtln_ods_customers"
```

**For Products (identical pattern):**
```sql
SELECT COALESCE(
  TO_CHAR(MAX("last_modified_timestamp"), 'YYYY-MM-DD HH24:MI:SS'),
  '1990-01-01 00:00:00'
) AS HWM
FROM "mtln_ods_products"
```

**For Sales (different timestamp column):**
```sql
SELECT COALESCE(
  TO_CHAR(MAX("order_date"), 'YYYY-MM-DD HH24:MI:SS'),
  '1990-01-01 00:00:00'
) AS HWM
FROM "mtln_ods_sales"
```

**For Performance (different timestamp column):**
```sql
SELECT COALESCE(
  TO_CHAR(MAX("performance_date"), 'YYYY-MM-DD HH24:MI:SS'),
  '1990-01-01 00:00:00'
) AS HWM
FROM "mtln_ods_performance"
```

---

### 6.4 Incremental Load from Parquet Files

#### 6.4.1 Component: Snowflake Load (Copy Into)

**For Campaigns:**
```yaml
Load Bronze Campaigns:
  type: "snowflake-load"
  transitions:
    success:
      - "Bronze to Silver"
  parameters:
    componentName: "Load Bronze Campaigns"
    warehouse: "[Environment Default]"
    database: "[Environment Default]"
    schema: "[Environment Default]"
    targetTable: "mtln_bronze_campaigns"
    stage: "@CAMPAIGN_STAGE"
    filePattern: "campaigns_.*\\.parquet"
    fileFormat: "(TYPE = 'PARQUET')"
    loadOptions:
      - "MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE"
      - "PURGE = FALSE"
    onError: "ABORT_STATEMENT"
```

**SQL Generated:**
```sql
COPY INTO mtln_bronze_campaigns
FROM @CAMPAIGN_STAGE
FILES = ('campaigns_20251221.parquet')
FILE_FORMAT = (TYPE = 'PARQUET')
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE
PURGE = FALSE
ON_ERROR = ABORT_STATEMENT;
```

**Note:** Parquet files should already be filtered at source to include only records where `last_modified_date > HWM`. If full files are loaded, add post-load filtering.

---

### 6.5 Incremental Filter in Bronze-to-Silver

**If Parquet files contain full data, filter in transformation:**

#### 6.5.1 Bronze-to-Silver with Incremental Filter

```yaml
Bronze Campaigns Input:
  type: "table-input"
  parameters:
    componentName: "Bronze Campaigns Input"
    database: "[Environment Default]"
    schema: "[Environment Default]"
    targetTable: "mtln_bronze_campaigns"
    columnNames:
      - "campaign_id"
      - "campaign_name"
      - "campaign_type"
      - "start_date"
      - "end_date"
      - "budget"
      - "status"
      - "objective"
      - "last_modified_date"
    whereClause: |
      last_modified_date > TO_TIMESTAMP('${HWM_campaigns}', 'YYYY-MM-DD HH24:MI:SS')
```

**Filter Component (Alternative):**
```yaml
Filter Incremental:
  type: "filter"
  sources:
    - "Bronze Campaigns Input"
  parameters:
    componentName: "Filter Incremental"
    filterConditions:
      - - "last_modified_date"
        - "Is"
        - "Greater than"
        - "TO_TIMESTAMP('${HWM_campaigns}', 'YYYY-MM-DD HH24:MI:SS')"
    combineCondition: "And"
```

---

### 6.6 Full Refresh Strategy (Channels)

**For small, slowly changing dimensions:**

**Channels Load (No High Water Mark):**
```yaml
Load Bronze Channels:
  type: "snowflake-load"
  parameters:
    componentName: "Load Bronze Channels"
    targetTable: "mtln_bronze_channels"
    stage: "@CHANNEL_STAGE"
    filePattern: "channels_.*\\.parquet"
    truncateTable: "Yes"  # Full refresh
    fileFormat: "(TYPE = 'PARQUET')"
```

**Rationale:**
- âœ… Small table (< 100 rows)
- âœ… Changes infrequent
- âœ… Full refresh takes < 1 second
- âœ… Simpler than incremental logic

---

### 6.7 Merge Logic in Silver Layer

#### 6.7.1 Table Update Component (MERGE)

**For Campaigns:**
```yaml
Update ODS Campaigns:
  type: "table-update"
  sources:
    - "Cleaned Campaigns"
  parameters:
    componentName: "Update ODS Campaigns"
    warehouse: "[Environment Default]"
    database: "[Environment Default]"
    schema: "[Environment Default]"
    targetTable: "mtln_ods_campaigns"
    targetAlias: "target"
    sourceAlias: "input"
    joinExpression:
      - - '"input"."campaign_id" = "target"."campaign_id"'
        - "Case"
    whenMatched:
      - - "true"
        - "Update"
    updateMapping:
      - - "campaign_name"
        - "campaign_name"
      - - "campaign_type"
        - "campaign_type"
      - - "start_date"
        - "start_date"
      - - "end_date"
        - "end_date"
      - - "budget"
        - "budget"
      - - "status"
        - "status"
      - - "objective"
        - "objective"
      - - "last_modified_timestamp"
        - "last_modified_timestamp"
    includeNotMatched: "Yes"
    insertMapping:
      - - "campaign_id"
        - "campaign_id"
      - - "campaign_name"
        - "campaign_name"
      - - "campaign_type"
        - "campaign_type"
      - - "start_date"
        - "start_date"
      - - "end_date"
        - "end_date"
      - - "budget"
        - "budget"
      - - "status"
        - "status"
      - - "objective"
        - "objective"
      - - "last_modified_timestamp"
        - "last_modified_timestamp"
```

**SQL Generated:**
```sql
MERGE INTO mtln_ods_campaigns AS target
USING (
    SELECT 
        campaign_id,
        campaign_name,
        campaign_type,
        start_date,
        end_date,
        budget,
        status,
        objective,
        last_modified_timestamp
    FROM cleaned_campaigns_temp
) AS input
ON input.campaign_id = target.campaign_id
WHEN MATCHED THEN 
    UPDATE SET 
        campaign_name = input.campaign_name,
        campaign_type = input.campaign_type,
        start_date = input.start_date,
        end_date = input.end_date,
        budget = input.budget,
        status = input.status,
        objective = input.objective,
        last_modified_timestamp = input.last_modified_timestamp,
        ods_updated_timestamp = CURRENT_TIMESTAMP()
WHEN NOT MATCHED THEN
    INSERT (
        campaign_id,
        campaign_name,
        campaign_type,
        start_date,
        end_date,
        budget,
        status,
        objective,
        last_modified_timestamp
    )
    VALUES (
        input.campaign_id,
        input.campaign_name,
        input.campaign_type,
        input.start_date,
        input.end_date,
        input.budget,
        input.status,
        input.objective,
        input.last_modified_timestamp
    );
```

**Key Points:**
- âœ… **Surrogate key NOT included** - Sequence DEFAULT handles it
- âœ… **UPDATE:** Existing records get new values
- âœ… **INSERT:** New records get surrogate key from sequence
- âœ… `ods_updated_timestamp` automatically set on UPDATE

---

### 6.8 Incremental Performance Comparison

**Scenario:** 1 million campaign records, 10K changes per day

| Strategy | Rows Processed | Time | Cost |
|----------|----------------|------|------|
| **Full Refresh** | 1,000,000 | 5 min | $0.50 |
| **Incremental** | 10,000 | 8 sec | $0.02 |
| **Savings** | 99% fewer | 97% faster | 96% cheaper |

**Recommendation:** Use incremental for:
- âœ… Tables > 100K rows
- âœ… Daily change rate < 10%
- âœ… Source provides `last_modified` timestamp

---

### 6.9 Initial Load vs. Incremental Load

#### Initial Load (First Run)
- High water mark = '1990-01-01 00:00:00' (default)
- All records loaded
- ODS tables populated from scratch

#### Subsequent Loads
- High water mark = MAX(timestamp) from previous load
- Only new/changed records loaded
- MERGE updates existing, inserts new

#### Reset High Water Mark (Reprocess)
```sql
-- Manual reset to reload all data
UPDATE pipeline_variable 
SET default_value = '1990-01-01 00:00:00'
WHERE variable_name = 'HWM_campaigns';
```

Or truncate ODS table:
```sql
TRUNCATE TABLE mtln_ods_campaigns;
-- Next run will be initial load
```

---

## 7. Technical Specifications

### 7.1 Naming Conventions

#### 7.1.1 Database Objects

**Pattern:** `<prefix>_<layer>_<entity>_<suffix>`

| Object Type | Pattern | Example |
|-------------|---------|--------|
| **Stages** | `<ENTITY>_STAGE` | `CAMPAIGN_STAGE` |
| **Bronze Tables** | `mtln_bronze_<entity>` | `mtln_bronze_campaigns` |
| **ODS Tables** | `mtln_ods_<entity>` | `mtln_ods_campaigns` |
| **Sequences** | `mtln_ods_<entity>_seq` | `mtln_ods_campaigns_seq` |
| **Dimension Views** | `mtln_dim_<entity>` | `mtln_dim_campaign` |
| **Fact Views** | `mtln_fact_<entity>` | `mtln_fact_sales` |
| **Error Log** | `mtln_error_log` | `mtln_error_log` |

**Prefix Explanation:**
- `mtln_` = Matillion (identifies objects created by this project)
- Prevents naming conflicts with existing objects
- Easy to identify and manage

#### 7.1.2 Columns

**Surrogate Keys:**
- ODS: `surrogate_key`
- Gold Dimensions: `dim_<entity>_sk`
- Gold Facts: `fact_<entity>_sk`

**Natural Keys:**
- ODS: `<entity>_id` (e.g., `campaign_id`)
- Gold: `natural_key`

**Foreign Keys:**
- ODS: `<entity>_surrogate_key` (e.g., `customer_surrogate_key`)
- Gold: `dim_<entity>_sk` (e.g., `dim_customer_sk`)

**Timestamps:**
- Source: `last_modified_date` or `last_modified_timestamp`
- Bronze: `bronze_load_timestamp`
- ODS: `ods_created_timestamp`, `ods_updated_timestamp`
- Gold: `effective_timestamp`

**Descriptive Columns:**
- ODS: `<column_name>` (e.g., `campaign_name`)
- Gold: `<entity>_<column_name>` (e.g., `campaign_name`, `customer_email`)

#### 7.1.3 Pipeline Files

**Pattern:** `<purpose>-<layer>-<action>.{orch|tran}.yaml`

| Type | Pattern | Example |
|------|---------|--------|
| **Master Orchestration** | `master-<flow>.orch.yaml` | `master-stage-to-gold.orch.yaml` |
| **Transformation** | `trans-<source>-to-<target>.tran.yaml` | `trans-bronze-to-silver.tran.yaml` |
| **DDL** | `create-<objects>-<layer>.orch.yaml` | `create-tables-silver.orch.yaml` |
| **Utility** | `<purpose>-<entity>.orch.yaml` | `data-quality-validation.orch.yaml` |

#### 7.1.4 Pipeline Variables

**Pattern:** `<PREFIX>_<entity>`

| Variable Type | Pattern | Example |
|---------------|---------|--------|
| **High Water Mark** | `HWM_<entity>` | `HWM_campaigns` |
| **Config** | `<PURPOSE>_<entity>` | `DATABASE_NAME`, `SCHEMA_NAME` |
| **OAuth** | `<System>OAuthName` | `SalesforceOAuthName` |

---

### 7.2 Data Types and Constraints

#### 7.2.1 Standard Data Type Mapping

| Logical Type | Snowflake Type | Size | Example |
|--------------|----------------|------|--------|
| **ID (Natural Key)** | VARCHAR | 50 | `campaign_id VARCHAR(50)` |
| **Surrogate Key** | NUMBER | (12,0) | `surrogate_key NUMBER(12,0)` |
| **Name/Description** | VARCHAR | 500 | `campaign_name VARCHAR(500)` |
| **Email** | VARCHAR | 255 | `email VARCHAR(255)` |
| **Phone** | VARCHAR | 20 | `phone VARCHAR(20)` |
| **Short Code** | VARCHAR | 20 | `status VARCHAR(20)` |
| **Currency** | NUMBER | (18,2) | `budget NUMBER(18,2)` |
| **Quantity** | NUMBER | (10,0) | `quantity NUMBER(10,0)` |
| **Percentage** | NUMBER | (5,2) | `margin_percent NUMBER(5,2)` |
| **Date Only** | DATE | - | `order_date DATE` |
| **Timestamp** | TIMESTAMP_NTZ | - | `last_modified_timestamp TIMESTAMP_NTZ` |
| **Boolean** | BOOLEAN | - | `is_active BOOLEAN` |
| **Large Metric** | NUMBER | (18,0) | `impressions NUMBER(18,0)` |

**Why TIMESTAMP_NTZ:**
- NTZ = No Time Zone
- Source systems may be in different time zones
- Store as UTC, convert in application layer
- Consistent across all environments

#### 7.2.2 Constraints by Layer

**Bronze Layer:**
```sql
CREATE TABLE mtln_bronze_campaigns (
    campaign_id               VARCHAR(50),        -- No NOT NULL
    campaign_name             VARCHAR(500),
    budget                    NUMBER(18,2),
    last_modified_date        TIMESTAMP_NTZ,
    bronze_load_timestamp     TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
    -- NO primary key, NO unique constraints (allow duplicates)
);
```

**Silver/ODS Layer:**
```sql
CREATE TABLE mtln_ods_campaigns (
    surrogate_key             NUMBER(12,0) DEFAULT mtln_ods_campaigns_seq.NEXTVAL,
    campaign_id               VARCHAR(50)    NOT NULL,
    campaign_name             VARCHAR(500)   NOT NULL,
    budget                    NUMBER(18,2),
    last_modified_timestamp   TIMESTAMP_NTZ  NOT NULL,
    ods_created_timestamp     TIMESTAMP_NTZ  DEFAULT CURRENT_TIMESTAMP(),
    ods_updated_timestamp     TIMESTAMP_NTZ  DEFAULT CURRENT_TIMESTAMP(),
    CONSTRAINT pk_ods_campaigns PRIMARY KEY (surrogate_key),
    CONSTRAINT uk_ods_campaigns UNIQUE (campaign_id)
);
```

**Constraints:**
- âœ… Primary Key on surrogate_key
- âœ… Unique constraint on natural key
- âœ… NOT NULL on critical columns
- âœ… DEFAULT values for timestamps

**Gold Layer (Views):**
- No constraints (inherited from ODS)
- Views cannot have constraints
- Constraints enforced at ODS level

---

### 7.3 Indexing and Clustering Strategy

#### 7.3.1 Clustering Keys (Snowflake)

**Purpose:** Optimize query performance by physically ordering data

**ODS Tables:**

```sql
-- Campaigns (query by date, campaign_id)
ALTER TABLE mtln_ods_campaigns 
CLUSTER BY (last_modified_timestamp, campaign_id);

-- Customers (query by segment, tier)
ALTER TABLE mtln_ods_customers 
CLUSTER BY (segment, tier, customer_id);

-- Products (query by category)
ALTER TABLE mtln_ods_products 
CLUSTER BY (category, subcategory, product_id);

-- Sales (query by date, customer)
ALTER TABLE mtln_ods_sales 
CLUSTER BY (order_date, customer_surrogate_key);

-- Performance (query by date, campaign)
ALTER TABLE mtln_ods_performance 
CLUSTER BY (performance_date, campaign_surrogate_key);

-- Channels (small table, no clustering needed)
-- No clustering for tables < 1 million rows
```

**Performance Impact:**
- âš¡ 50-80% faster queries (range scans)
- âš¡ Automatic maintenance by Snowflake
- âš¡ No manual index management

**Clustering Best Practices:**
1. Cluster on columns used in WHERE, JOIN, ORDER BY
2. Put most selective column first
3. Limit to 3-4 columns
4. High-cardinality first, low-cardinality last

#### 7.3.2 Search Optimization (Snowflake Feature)

**For point lookups on ODS:**
```sql
ALTER TABLE mtln_ods_campaigns 
ADD SEARCH OPTIMIZATION ON EQUALITY(campaign_id);

ALTER TABLE mtln_ods_customers 
ADD SEARCH OPTIMIZATION ON EQUALITY(customer_id), EQUALITY(email);

ALTER TABLE mtln_ods_products 
ADD SEARCH OPTIMIZATION ON EQUALITY(product_id), EQUALITY(sku);
```

**Use Case:** Fast lookups by exact match (e.g., "Find campaign CMP-001")

**Cost:** Additional storage + compute (evaluate based on query patterns)

---

### 7.4 Performance Optimizations

#### 7.4.1 Warehouse Sizing

**Recommendations:**

| Workload | Warehouse Size | Use Case |
|----------|----------------|----------|
| **ETL (Master Pipeline)** | MEDIUM | Data loading, transformations |
| **Ad-hoc Queries** | SMALL | Business user queries |
| **BI Tool Queries** | LARGE | Dashboard refreshes, reports |
| **Data Quality Validation** | SMALL | Validation queries |

**Auto-Suspend/Resume:**
```sql
ALTER WAREHOUSE MATILLION_ETL_WH SET 
    AUTO_SUSPEND = 300,      -- 5 minutes idle
    AUTO_RESUME = TRUE;

ALTER WAREHOUSE REPORTING_WH SET 
    AUTO_SUSPEND = 60,       -- 1 minute idle
    AUTO_RESUME = TRUE;
```

**Cost Savings:** 60-70% compute cost reduction (from context.md)

#### 7.4.2 Query Optimization

**Use Result Cache:**
```sql
-- Snowflake caches results for 24 hours
-- Identical queries return instantly
SELECT * FROM mtln_fact_sales WHERE dim_date_sk = 20251220;
-- Second run: < 1 second (from cache)
```

**Partition Pruning:**
```sql
-- Clustering enables partition pruning
SELECT SUM(revenue)
FROM mtln_fact_sales
WHERE dim_date_sk BETWEEN 20251201 AND 20251231;
-- Snowflake scans only December partitions
```

**Projection Pushdown:**
```sql
-- Select only needed columns
SELECT campaign_name, campaign_budget  -- Good
FROM mtln_dim_campaign;

SELECT *  -- Avoid (reads all columns)
FROM mtln_dim_campaign;
```

#### 7.4.3 Parallel Execution

**In Master Orchestration:**
- Create sequences: 6 parallel
- Create ODS tables: 6 parallel
- Set high water marks: 6 parallel
- Load Bronze: 6 parallel

**Benefits:**
- 6x faster than sequential
- Better warehouse utilization
- Reduced total execution time

**Matillion Implementation:**
```yaml
# Parallel components have no transitions between them
Create Sequence Campaigns:
  type: "sql-script"
  transitions:
    success:
      - "Create ODS Table Campaigns"  # Next stage
  # No dependency on other Create Sequence components

Create Sequence Customers:
  type: "sql-script"
  transitions:
    success:
      - "Create ODS Table Customers"
  # Runs in parallel with Create Sequence Campaigns
```

#### 7.4.4 Data Retention for Cost Savings

```sql
-- Set retention periods
ALTER TABLE mtln_bronze_campaigns SET DATA_RETENTION_TIME_IN_DAYS = 14;
ALTER TABLE mtln_ods_campaigns SET DATA_RETENTION_TIME_IN_DAYS = 30;

-- Time Travel available within retention period
SELECT * FROM mtln_ods_campaigns AT(OFFSET => -3600);  -- 1 hour ago
```

**Cost Impact:** 25% storage savings (from context.md)

---

### 7.5 Security Specifications

#### 7.5.1 Role-Based Access Control (RBAC)

**Role Hierarchy:**
```sql
-- Create roles
CREATE ROLE IF NOT EXISTS MATILLION_ETL_ROLE;
CREATE ROLE IF NOT EXISTS DATA_ENGINEER_ROLE;
CREATE ROLE IF NOT EXISTS ANALYST_ROLE;
CREATE ROLE IF NOT EXISTS BUSINESS_USER_ROLE;

-- Grant role hierarchy
GRANT ROLE MATILLION_ETL_ROLE TO ROLE SYSADMIN;
GRANT ROLE DATA_ENGINEER_ROLE TO ROLE SYSADMIN;
GRANT ROLE ANALYST_ROLE TO USER analyst_user;
GRANT ROLE BUSINESS_USER_ROLE TO USER business_user;
```

**Permissions by Layer:**

```sql
-- MATILLION_ETL_ROLE (full access for pipelines)
GRANT ALL ON DATABASE MATILLION_DB TO ROLE MATILLION_ETL_ROLE;
GRANT ALL ON SCHEMA DEV TO ROLE MATILLION_ETL_ROLE;
GRANT ALL ON ALL TABLES IN SCHEMA DEV TO ROLE MATILLION_ETL_ROLE;
GRANT ALL ON ALL VIEWS IN SCHEMA DEV TO ROLE MATILLION_ETL_ROLE;
GRANT USAGE ON WAREHOUSE MATILLION_WH TO ROLE MATILLION_ETL_ROLE;

-- DATA_ENGINEER_ROLE (read/write ODS and Bronze)
GRANT USAGE ON DATABASE MATILLION_DB TO ROLE DATA_ENGINEER_ROLE;
GRANT USAGE ON SCHEMA DEV TO ROLE DATA_ENGINEER_ROLE;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA DEV TO ROLE DATA_ENGINEER_ROLE;
GRANT SELECT ON ALL VIEWS IN SCHEMA DEV TO ROLE DATA_ENGINEER_ROLE;
GRANT USAGE ON WAREHOUSE MATILLION_WH TO ROLE DATA_ENGINEER_ROLE;

-- ANALYST_ROLE (read Gold and Silver)
GRANT USAGE ON DATABASE MATILLION_DB TO ROLE ANALYST_ROLE;
GRANT USAGE ON SCHEMA DEV TO ROLE ANALYST_ROLE;
GRANT SELECT ON mtln_ods_* TO ROLE ANALYST_ROLE;
GRANT SELECT ON mtln_dim_* TO ROLE ANALYST_ROLE;
GRANT SELECT ON mtln_fact_* TO ROLE ANALYST_ROLE;
GRANT USAGE ON WAREHOUSE REPORTING_WH TO ROLE ANALYST_ROLE;

-- BUSINESS_USER_ROLE (read Gold only)
GRANT USAGE ON DATABASE MATILLION_DB TO ROLE BUSINESS_USER_ROLE;
GRANT USAGE ON SCHEMA DEV TO ROLE BUSINESS_USER_ROLE;
GRANT SELECT ON mtln_dim_* TO ROLE BUSINESS_USER_ROLE;
GRANT SELECT ON mtln_fact_* TO ROLE BUSINESS_USER_ROLE;
GRANT USAGE ON WAREHOUSE REPORTING_WH TO ROLE BUSINESS_USER_ROLE;
```

#### 7.5.2 Row-Level Security (Optional)

**For multi-tenant or restricted data:**
```sql
CREATE OR REPLACE ROW ACCESS POLICY customer_region_policy
AS (region VARCHAR) RETURNS BOOLEAN ->
  CASE 
    WHEN CURRENT_ROLE() = 'ADMIN_ROLE' THEN TRUE
    WHEN CURRENT_ROLE() = 'US_ANALYST_ROLE' AND region = 'US' THEN TRUE
    WHEN CURRENT_ROLE() = 'EU_ANALYST_ROLE' AND region = 'EU' THEN TRUE
    ELSE FALSE
  END;

ALTER TABLE mtln_ods_customers 
ADD ROW ACCESS POLICY customer_region_policy ON (region);
```

#### 7.5.3 Column-Level Security (Masking)

**For PII data:**
```sql
CREATE OR REPLACE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR ->
  CASE
    WHEN CURRENT_ROLE() IN ('ADMIN_ROLE', 'DATA_ENGINEER_ROLE') THEN val
    ELSE CONCAT(LEFT(val, 3), '***@***.com')
  END;

ALTER TABLE mtln_ods_customers 
MODIFY COLUMN email SET MASKING POLICY email_mask;

-- Business users see: joh***@***.com
-- Engineers see: john.doe@email.com
```

---

### 7.6 Monitoring and Logging

#### 7.6.1 Query History Tracking

```sql
-- Monitor pipeline queries
SELECT 
    query_id,
    query_text,
    user_name,
    role_name,
    warehouse_name,
    execution_status,
    total_elapsed_time / 1000 AS elapsed_seconds,
    rows_produced,
    bytes_scanned,
    start_time
FROM snowflake.account_usage.query_history
WHERE user_name = 'MATILLION_USER'
  AND start_time >= DATEADD('hour', -24, CURRENT_TIMESTAMP())
ORDER BY start_time DESC;
```

#### 7.6.2 Warehouse Usage Monitoring

```sql
-- Track compute costs
SELECT 
    warehouse_name,
    SUM(credits_used) AS total_credits,
    SUM(credits_used) * 3 AS estimated_cost_usd  -- $3/credit example
FROM snowflake.account_usage.warehouse_metering_history
WHERE start_time >= DATEADD('day', -30, CURRENT_TIMESTAMP())
GROUP BY warehouse_name
ORDER BY total_credits DESC;
```

#### 7.6.3 Data Volume Tracking

```sql
-- Monitor table growth
SELECT 
    table_schema,
    table_name,
    row_count,
    bytes / (1024*1024*1024) AS size_gb,
    DATEDIFF('day', last_altered, CURRENT_TIMESTAMP()) AS days_since_update
FROM snowflake.account_usage.tables
WHERE table_schema = 'DEV'
  AND table_name LIKE 'mtln_%'
ORDER BY bytes DESC;
```

---

## 8. Deployment Specifications (Matillion)

### 8.1 Prerequisites

#### 8.1.1 Snowflake Setup

**Database and Schema:**
```sql
-- Create database
CREATE DATABASE IF NOT EXISTS MATILLION_DB;

-- Create schemas
CREATE SCHEMA IF NOT EXISTS MATILLION_DB.DEV;
CREATE SCHEMA IF NOT EXISTS MATILLION_DB.UAT;
CREATE SCHEMA IF NOT EXISTS MATILLION_DB.PROD;

USE SCHEMA MATILLION_DB.DEV;
```

**Warehouses:**
```sql
-- ETL warehouse
CREATE WAREHOUSE IF NOT EXISTS MATILLION_WH
    WAREHOUSE_SIZE = 'MEDIUM'
    AUTO_SUSPEND = 300
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
    COMMENT = 'Warehouse for Matillion ETL pipelines';

-- Reporting warehouse
CREATE WAREHOUSE IF NOT EXISTS REPORTING_WH
    WAREHOUSE_SIZE = 'LARGE'
    AUTO_SUSPEND = 60
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE
    COMMENT = 'Warehouse for BI queries and reporting';
```

**Roles and Users:**
```sql
-- Create Matillion service account
CREATE USER IF NOT EXISTS matillion_user
    PASSWORD = '<secure_password>'
    DEFAULT_ROLE = MATILLION_ETL_ROLE
    DEFAULT_WAREHOUSE = MATILLION_WH
    COMMENT = 'Service account for Matillion Data Productivity Cloud';

-- Grant role
GRANT ROLE MATILLION_ETL_ROLE TO USER matillion_user;
```

#### 8.1.2 Matillion Setup

**1. Create Snowflake Connection:**
- Navigate to: Project > Manage Connections
- Click: Add Connection
- Type: Snowflake
- Configuration:
  ```
  Connection Name: Snowflake_DEV
  Account: <your_account>.snowflakecomputing.com
  Warehouse: MATILLION_WH
  Database: MATILLION_DB
  Schema: DEV
  Authentication: Username/Password
  Username: matillion_user
  Password: <secure_password>
  ```
- Test Connection
- Save

**2. Set Environment Defaults:**
- Navigate to: Project > Environment Settings
- Set Default Connection: Snowflake_DEV
- Set Default Warehouse: MATILLION_WH
- Set Default Database: MATILLION_DB
- Set Default Schema: DEV

**3. Configure Git (Optional but Recommended):**
- Navigate to: Project > Git Settings
- Repository URL: `https://github.com/<org>/<repo>.git`
- Authentication: Personal Access Token
- Branch Strategy:
  - `main` â†’ PROD
  - `uat` â†’ UAT
  - `dev` â†’ DEV

---

### 8.2 Deployment Order

**Phase 1: Infrastructure (5 minutes)**
1. Create Internal Stages
2. Create Sequences
3. Create Bronze Tables
4. Create Silver/ODS Tables
5. Create Date Dimension Table

**Phase 2: Data Loading (5 minutes)**
1. Populate Date Dimension
2. Load initial data to Bronze
3. Run Bronze-to-Silver transformation
4. Validate ODS data

**Phase 3: Gold Layer (2 minutes)**
1. Create Gold Views (dimensions)
2. Create Gold Views (facts)
3. Apply clustering keys
4. Validate star schema

**Phase 4: Pipelines (3 minutes)**
1. Import/create Master Orchestration
2. Import/create transformations
3. Set pipeline variables
4. Test individual components
5. Test end-to-end execution

**Total Deployment Time: ~15 minutes**

---

### 8.3 Step-by-Step Deployment (Matillion)

#### 8.3.1 Create Infrastructure Objects

**Option A: Using Matillion DDL Pipeline**

**File:** `create-infrastructure.orch.yaml`

```yaml
type: "orchestration"
version: "1.0"
pipeline:
  components:
    Start:
      type: "start"
      transitions:
        unconditional:
          - "Create Stages"
      parameters:
        componentName: "Start"
    
    Create Stages:
      type: "sql-script"
      transitions:
        success:
          - "Create Sequences"
      parameters:
        componentName: "Create Stages"
        sqlScript: |
          -- Internal Stages
          CREATE STAGE IF NOT EXISTS CAMPAIGN_STAGE;
          CREATE STAGE IF NOT EXISTS CUSTOMER_STAGE;
          CREATE STAGE IF NOT EXISTS PRODUCT_STAGE;
          CREATE STAGE IF NOT EXISTS SALES_STAGE;
          CREATE STAGE IF NOT EXISTS PERFORMANCE_STAGE;
          CREATE STAGE IF NOT EXISTS CHANNEL_STAGE;
    
    Create Sequences:
      type: "sql-script"
      transitions:
        success:
          - "Create Bronze Tables"
      parameters:
        componentName: "Create Sequences"
        sqlScript: |
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_campaigns_seq;
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_customers_seq;
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_products_seq;
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_sales_seq;
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_performance_seq;
          CREATE SEQUENCE IF NOT EXISTS mtln_ods_channels_seq;
    
    Create Bronze Tables:
      type: "sql-script"
      transitions:
        success:
          - "Create ODS Tables"
      parameters:
        componentName: "Create Bronze Tables"
        sqlScript: |
          CREATE TABLE IF NOT EXISTS mtln_bronze_campaigns (
              campaign_id VARCHAR(50),
              campaign_name VARCHAR(500),
              campaign_type VARCHAR(100),
              start_date DATE,
              end_date DATE,
              budget NUMBER(18,2),
              status VARCHAR(50),
              objective VARCHAR(500),
              last_modified_date TIMESTAMP_NTZ,
              bronze_load_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
          );
          -- Repeat for other Bronze tables...
    
    Create ODS Tables:
      type: "sql-script"
      transitions:
        success:
          - "Create Date Dimension"
      parameters:
        componentName: "Create ODS Tables"
        sqlScript: |
          CREATE TABLE IF NOT EXISTS mtln_ods_campaigns (
              surrogate_key NUMBER(12,0) DEFAULT mtln_ods_campaigns_seq.NEXTVAL,
              campaign_id VARCHAR(50) NOT NULL,
              campaign_name VARCHAR(500) NOT NULL,
              campaign_type VARCHAR(100),
              start_date DATE,
              end_date DATE,
              budget NUMBER(18,2),
              status VARCHAR(50),
              objective VARCHAR(500),
              last_modified_timestamp TIMESTAMP_NTZ NOT NULL,
              ods_created_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
              ods_updated_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
              CONSTRAINT pk_ods_campaigns PRIMARY KEY (surrogate_key),
              CONSTRAINT uk_ods_campaigns UNIQUE (campaign_id)
          );
          -- Repeat for other ODS tables...
    
    Create Date Dimension:
      type: "sql-script"
      transitions:
        success:
          - "Populate Date Dimension"
      parameters:
        componentName: "Create Date Dimension"
        sqlScript: |
          CREATE TABLE IF NOT EXISTS mtln_dim_date (
              date_key NUMBER(8,0) PRIMARY KEY,
              full_date DATE NOT NULL,
              year NUMBER(4,0) NOT NULL,
              quarter NUMBER(1,0) NOT NULL,
              month NUMBER(2,0) NOT NULL,
              month_name VARCHAR(20) NOT NULL,
              week NUMBER(2,0) NOT NULL,
              day_of_week NUMBER(1,0) NOT NULL,
              day_name VARCHAR(20) NOT NULL,
              day_of_month NUMBER(2,0) NOT NULL,
              day_of_year NUMBER(3,0) NOT NULL,
              is_weekend BOOLEAN NOT NULL,
              is_holiday BOOLEAN DEFAULT FALSE
          );
    
    Populate Date Dimension:
      type: "sql-script"
      parameters:
        componentName: "Populate Date Dimension"
        sqlScript: |
          INSERT INTO mtln_dim_date
          SELECT 
              TO_NUMBER(TO_CHAR(date_val, 'YYYYMMDD')) AS date_key,
              date_val AS full_date,
              YEAR(date_val) AS year,
              QUARTER(date_val) AS quarter,
              MONTH(date_val) AS month,
              MONTHNAME(date_val) AS month_name,
              WEEKOFYEAR(date_val) AS week,
              DAYOFWEEK(date_val) AS day_of_week,
              DAYNAME(date_val) AS day_name,
              DAYOFMONTH(date_val) AS day_of_month,
              DAYOFYEAR(date_val) AS day_of_year,
              CASE WHEN DAYOFWEEK(date_val) IN (0,6) THEN TRUE ELSE FALSE END AS is_weekend,
              FALSE AS is_holiday
          FROM (
              SELECT DATEADD('day', SEQ4(), '2020-01-01'::DATE) AS date_val
              FROM TABLE(GENERATOR(ROWCOUNT => 3653))
          ) dates
          WHERE date_val <= '2030-12-31'::DATE
            AND date_val NOT IN (SELECT full_date FROM mtln_dim_date);
```

**To Run in Matillion:**
1. Create new orchestration pipeline
2. Copy/paste the YAML above
3. Click **Run** button
4. Monitor execution in Run History
5. Verify objects created in Snowflake

**Option B: Using Matillion Visual Components**

Use dedicated [Create Table](https://docs.matillion.com/data-productivity-cloud/designer/docs/create-table/) components:

1. **Add Create Table Component**
   - Component Type: `create-table-v2`
   - Configure each table with DDL details

2. **Configure Parameters:**
   ```yaml
   Create ODS Campaigns:
     type: "create-table-v2"
     parameters:
       componentName: "Create ODS Campaigns"
       createReplace: "Create if not exists"
       database: "[Environment Default]"
       schema: "[Environment Default]"
       newTableName: "mtln_ods_campaigns"
       tableType: "Permanent"
       columns:
         - - "surrogate_key"
           - "NUMBER"
           - "12"
           - "0"
           - "mtln_ods_campaigns_seq.NEXTVAL"
           - "Yes"  # NOT NULL
           - "Yes"  # PRIMARY KEY
           - ""
         - - "campaign_id"
           - "VARCHAR"
           - "50"
           - ""
           - ""
           - "Yes"  # NOT NULL
           - "No"
           - ""
         # ... more columns
   ```

---

#### 8.3.2 Create Gold Views

**File:** `create-gold-views.orch.yaml`

```yaml
Create Dimension Views:
  type: "sql-script"
  transitions:
    success:
      - "Create Fact Views"
  parameters:
    componentName: "Create Dimension Views"
    sqlScript: |
      -- DIM_CAMPAIGN
      CREATE OR REPLACE VIEW mtln_dim_campaign AS
      SELECT 
          surrogate_key AS dim_campaign_sk,
          campaign_id AS natural_key,
          campaign_name,
          campaign_type,
          start_date AS campaign_start_date,
          end_date AS campaign_end_date,
          budget AS campaign_budget,
          status AS campaign_status,
          objective AS campaign_objective,
          last_modified_timestamp AS effective_timestamp,
          DATEDIFF('day', start_date, end_date) + 1 AS campaign_duration_days
      FROM mtln_ods_campaigns;
      
      -- DIM_CUSTOMER
      CREATE OR REPLACE VIEW mtln_dim_customer AS
      SELECT 
          surrogate_key AS dim_customer_sk,
          customer_id AS natural_key,
          customer_name,
          email AS customer_email,
          segment AS customer_segment,
          tier AS customer_tier,
          lifetime_value AS customer_lifetime_value
      FROM mtln_ods_customers;
      
      -- ... other dimensions

Create Fact Views:
  type: "sql-script"
  transitions:
    success:
      - "Apply Clustering"
  parameters:
    componentName: "Create Fact Views"
    sqlScript: |
      -- FACT_SALES
      CREATE OR REPLACE VIEW mtln_fact_sales AS
      SELECT 
          surrogate_key AS fact_sales_sk,
          customer_surrogate_key AS dim_customer_sk,
          product_surrogate_key AS dim_product_sk,
          campaign_surrogate_key AS dim_campaign_sk,
          TO_NUMBER(TO_CHAR(order_date, 'YYYYMMDD')) AS dim_date_sk,
          quantity,
          revenue,
          line_total
      FROM mtln_ods_sales;
      
      -- FACT_PERFORMANCE
      CREATE OR REPLACE VIEW mtln_fact_performance AS
      SELECT 
          surrogate_key AS fact_performance_sk,
          campaign_surrogate_key AS dim_campaign_sk,
          channel_surrogate_key AS dim_channel_sk,
          TO_NUMBER(TO_CHAR(performance_date, 'YYYYMMDD')) AS dim_date_sk,
          impressions,
          clicks,
          cost,
          revenue,
          roas
      FROM mtln_ods_performance;

Apply Clustering:
  type: "sql-script"
  parameters:
    componentName: "Apply Clustering"
    sqlScript: |
      ALTER TABLE mtln_ods_sales CLUSTER BY (order_date, customer_surrogate_key);
      ALTER TABLE mtln_ods_performance CLUSTER BY (performance_date, campaign_surrogate_key);
      ALTER TABLE mtln_ods_campaigns CLUSTER BY (last_modified_timestamp, campaign_id);
```

---

#### 8.3.3 Import Master Pipeline

**Method 1: Git Import (Recommended)**

1. **Commit pipelines to Git:**
   ```bash
   git add master-stage-to-gold.orch.yaml
   git add trans-bronze-to-silver.tran.yaml
   git add trans-silver-to-gold.tran.yaml
   git commit -m "Add master pipeline and transformations"
   git push origin dev
   ```

2. **Pull in Matillion:**
   - Navigate to: Project > Git
   - Click: **Pull from Remote**
   - Select branch: `dev`
   - Pipelines appear in project tree

**Method 2: Manual Creation**

1. **Create Master Orchestration:**
   - Click: **+ New Pipeline**
   - Type: Orchestration
   - Name: `master-stage-to-gold`

2. **Add Components:**
   - Drag [Start](https://docs.matillion.com/data-productivity-cloud/designer/docs/start/) component
   - Add [SQL Script](https://docs.matillion.com/data-productivity-cloud/designer/docs/sql-script/) for sequences
   - Add [Query to Scalar Variable](https://docs.matillion.com/data-productivity-cloud/designer/docs/query-to-scalar-variable/) for high water marks
   - Add [Snowflake Load](https://docs.matillion.com/data-productivity-cloud/designer/docs/snowflake-load/) for Bronze
   - Add [Run Transformation](https://docs.matillion.com/data-productivity-cloud/designer/docs/run-transformation/) for Silver/Gold

3. **Configure Transitions:**
   - Connect components with success/failure paths
   - Parallel components have no dependencies

4. **Set Pipeline Variables:**
   - Click: Pipeline > Variables
   - Add variables:
     ```
     HWM_campaigns: TEXT, COPIED, PRIVATE, default "1990-01-01 00:00:00"
     HWM_customers: TEXT, COPIED, PRIVATE, default "1990-01-01 00:00:00"
     ... etc
     ```

**Method 3: Import from File**

1. **Export YAML files** from another Matillion project
2. **Import in target project:**
   - Navigate to: Project > Import
   - Select files
   - Click: **Import**
   - Resolve any connection references

---

#### 8.3.4 Configure Pipeline Variables

**In Master Pipeline:**
1. Open `master-stage-to-gold.orch.yaml`
2. Click: **Variables** tab (bottom panel)
3. Verify/Add:
   ```
   Variable Name: HWM_campaigns
   Type: TEXT
   Scope: COPIED
   Visibility: PRIVATE
   Default Value: 1990-01-01 00:00:00
   Description: Maximum last_modified_timestamp from mtln_ods_campaigns
   ```
4. Repeat for all high water mark variables

---

#### 8.3.5 Test Deployment

**Component Testing:**

1. **Test Query to Scalar Variable:**
   - Right-click component
   - Select: **Sample Component**
   - Verify variable populated
   - Check value in **Output** panel

2. **Test Table Input:**
   - Right-click Bronze table input
   - Select: **Sample Component**
   - Verify 10 rows displayed
   - Check for data quality issues

3. **Test Transformation:**
   - Open transformation pipeline
   - Click: **Run** (uses sample data)
   - Verify output in final component

**End-to-End Testing:**

1. **Prepare Test Data:**
   ```sql
   -- Upload test Parquet files to stages
   PUT file:///path/to/campaigns_20251221.parquet @CAMPAIGN_STAGE;
   PUT file:///path/to/customers_20251221.parquet @CUSTOMER_STAGE;
   ```

2. **Run Master Pipeline:**
   - Click: **Run** button
   - Monitor in Run History
   - Check execution time (~15 min expected)

3. **Validate Results:**
   ```sql
   -- Check row counts
   SELECT 'Bronze' AS layer, COUNT(*) AS row_count FROM mtln_bronze_campaigns
   UNION ALL
   SELECT 'ODS', COUNT(*) FROM mtln_ods_campaigns
   UNION ALL
   SELECT 'Gold', COUNT(*) FROM mtln_dim_campaign;
   
   -- Check data quality
   SELECT COUNT(*) AS duplicates
   FROM mtln_ods_campaigns
   GROUP BY campaign_id
   HAVING COUNT(*) > 1;
   -- Expected: 0
   ```

---

### 8.4 Environment Promotion (DEV â†’ UAT â†’ PROD)

#### 8.4.1 Using Git Branching

**Branch Strategy:**
```
dev (development)  â†’  Merge to  â†’  uat (testing)  â†’  Merge to  â†’  main (production)
```

**Promotion Process:**

1. **DEV to UAT:**
   ```bash
   git checkout uat
   git merge dev
   git push origin uat
   ```

2. **In Matillion UAT Environment:**
   - Switch branch to `uat`
   - Pull latest changes
   - Update environment-specific variables:
     ```yaml
     DATABASE_NAME: MATILLION_DB
     SCHEMA_NAME: UAT  # Changed from DEV
     ```
   - Run pipelines
   - Perform UAT testing

3. **UAT to PROD:**
   ```bash
   git checkout main
   git merge uat
   git tag -a v1.0.0 -m "Production release 1.0.0"
   git push origin main --tags
   ```

4. **In Matillion PROD Environment:**
   - Switch branch to `main`
   - Pull tag `v1.0.0`
   - Update variables:
     ```yaml
     SCHEMA_NAME: PROD
     ```
   - Test with sample data
   - Schedule pipelines

---

### 8.5 Scheduling (Production)

#### 8.5.1 Configure Schedule in Matillion

**Daily Batch Schedule:**

1. **Open Master Pipeline**
2. **Click: Schedule tab**
3. **Add Schedule:**
   ```
   Schedule Name: Daily ETL Run
   Frequency: Daily
   Start Time: 02:00 AM (local time)
   Time Zone: America/New_York
   Days: Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday
   Enabled: Yes
   ```

4. **Set Alerts:**
   ```
   On Failure: Email to data-engineering@company.com
   On Success: Email summary to ops@company.com
   ```

5. **Set Timeout:**
   ```
   Max Execution Time: 30 minutes
   Action on Timeout: Fail and alert
   ```

---

### 8.6 Rollback Procedures

#### 8.6.1 Pipeline Rollback (Git)

**Revert to Previous Version:**
```bash
# Find previous commit
git log --oneline

# Revert to specific commit
git revert <commit_hash>
git push origin main

# In Matillion: Pull latest
```

#### 8.6.2 Data Rollback (Snowflake Time Travel)

**Restore Table to Previous State:**
```sql
-- View table 1 hour ago
SELECT * FROM mtln_ods_campaigns AT(OFFSET => -3600);

-- Restore table
CREATE OR REPLACE TABLE mtln_ods_campaigns AS
SELECT * FROM mtln_ods_campaigns AT(TIMESTAMP => '2025-12-21 01:00:00'::TIMESTAMP);

-- Alternative: Clone before restore
CREATE TABLE mtln_ods_campaigns_backup CLONE mtln_ods_campaigns;
```

**Time Travel Limits:**
- Standard tables: 1 day (default)
- With DATA_RETENTION_TIME_IN_DAYS = 30: 30 days
- Enterprise Edition required for > 1 day

---

### 8.7 Troubleshooting Guide

#### 8.7.1 Common Issues

| Issue | Symptom | Solution |
|-------|---------|----------|
| **Connection Timeout** | Pipeline fails at component | Check warehouse status, increase timeout |
| **Zero Rows Loaded** | Bronze table empty | Verify files in stage, check file pattern |
| **Duplicate Key Error** | MERGE fails in ODS | Check deduplication logic in Bronze-to-Silver |
| **View Not Found** | Gold queries fail | Verify ODS tables exist, recreate views |
| **Variable Not Set** | Component references undefined variable | Check variable scope (COPIED vs SHARED) |
| **Sequence Error** | Surrogate key NULL | Verify sequence exists, check DEFAULT clause |

#### 8.7.2 Debug Steps

**1. Enable Detailed Logging:**
- Matillion: Project > Settings > Logging Level > DEBUG

**2. Sample Components:**
- Right-click any component
- Select: Sample Component
- View output data

**3. Check Snowflake Query History:**
```sql
SELECT 
    query_text,
    error_message,
    execution_status
FROM snowflake.account_usage.query_history
WHERE user_name = 'MATILLION_USER'
  AND execution_status = 'FAILED'
ORDER BY start_time DESC
LIMIT 10;
```

**4. Validate Pipeline YAML:**
- Check indentation (YAML is whitespace-sensitive)
- Verify component type exists
- Confirm parameter names match schema

---

### 8.8 Post-Deployment Checklist

**Infrastructure:**
- [ ] All 6 stages created
- [ ] All 6 sequences created
- [ ] All 6 Bronze tables created
- [ ] All 6 ODS tables created
- [ ] Date dimension created and populated
- [ ] All 7 Gold views created
- [ ] Clustering keys applied

**Pipelines:**
- [ ] Master orchestration pipeline imported
- [ ] Bronze-to-Silver transformation imported
- [ ] Silver-to-Gold transformation imported
- [ ] All pipeline variables configured
- [ ] Connections tested

**Data Quality:**
- [ ] Test data loaded successfully
- [ ] Row counts validated
- [ ] No duplicates in ODS
- [ ] Referential integrity verified
- [ ] Gold views return data

**Operations:**
- [ ] Schedule configured (PROD only)
- [ ] Alerts configured
- [ ] Monitoring queries set up
- [ ] Git repository linked
- [ ] Documentation updated

**Security:**
- [ ] Roles and permissions configured
- [ ] Service account created
- [ ] Network policies applied (if required)
- [ ] Audit logging enabled

---

**Document Status:** Complete - All Sections Detailed  
**Last Updated:** 2025-12-21  
**Implementation Ready:** Yes  
**Deployment Time:** ~15 minutes (infrastructure + pipelines)  
**Estimated First Run:** ~15 minutes (with test data)

**For business context:**  
â†’ See **[ARCHITECTURE-HLD.md](ARCHITECTURE-HLD.md)**

---

**Document Status:** Complete - Dimensional Model and Architecture  
**Last Updated:** 2025-12-21  
**Implementation Ready:** Yes

**For business context and high-level overview:**  
â†’ See **[ARCHITECTURE-HLD.md](ARCHITECTURE-HLD.md)**